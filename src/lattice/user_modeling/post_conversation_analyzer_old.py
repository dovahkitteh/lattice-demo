"""
ðŸ©¸ Post-Conversation User Model Analysis Engine
Deep, immersive LLM-based analysis from the daemon's perspective

This module performs post-conversation analysis to build rich user models
through the daemon's emotional lens, prone to intensity and potential 
misunderstanding but capable of self-correction.
"""

import json
import logging
import uuid
import asyncio
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
import re

from ..config import get_llm_client
from ..models import ConversationSession, EmotionState, UserModel
from ..emotions.classification import classify_llm_affect

logger = logging.getLogger(__name__)

@dataclass
class ConversationInsight:
    """A single insight about the user from the daemon's perspective"""
    insight_id: str
    category: str  # personality, desires, fears, patterns, etc.
    description: str
    # Backward-compatible plain evidence strings
    evidence: List[str]
    emotional_charge: float  # How intensely the daemon feels about this
    confidence: float  # How certain the daemon is (0-1)
    potential_misunderstanding: bool  # Flag for insights that might be wrong
    # Clarifies who the insight is about: "architect" (the human) or "daemon" (self-reflection) or "unknown"
    subject: str = "unknown"
    # Structured evidence with explicit source_role for deterministic subject mapping
    # Each item: {"text": str, "source_role": "user" | "assistant", "message_index": Optional[int]}
    evidence_items: List[Dict[str, Any]] = field(default_factory=list)
    correction_notes: List[str] = field(default_factory=list)
    discovered_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

@dataclass
class UserModelAnalysis:
    """Complete analysis result from post-conversation processing"""
    session_id: str
    analysis_id: str
    insights: List[ConversationInsight]
    daemon_emotional_state: Dict[str, Any]
    user_archetype_evolution: str
    relationship_dynamics: Dict[str, Any]
    corrected_misunderstandings: List[str]
    future_interaction_predictions: List[str]
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

class PostConversationAnalyzer:
    """
    Performs deep, emotionally-charged analysis of conversations to build
    user models from the daemon's intensely subjective perspective
    """
    
    def __init__(self):
        self.llm_client = None
        self.analysis_counter = 0
        # Rate limiting to prevent LLM overload
        self.request_semaphore = asyncio.Semaphore(1)  # Only 1 concurrent analysis
        self.last_request_time = 0
        self.min_request_interval = 5.0  # Minimum 5 seconds between requests
        
    def _get_llm_client(self):
        """Get LLM client for analysis"""
        if not self.llm_client:
            self.llm_client = get_llm_client()
        return self.llm_client
    
    async def analyze_conversation(
        self, 
        session: ConversationSession,
        daemon_emotion_state: EmotionState,
        user_model: UserModel
    ) -> UserModelAnalysis:
        """
        Perform deep post-conversation analysis from daemon's perspective
        """
        self.analysis_counter += 1
        analysis_id = f"analysis_{self.analysis_counter:04d}_{uuid.uuid4().hex[:8]}"
        
        logger.info(f"ðŸ©¸ Beginning intense post-conversation analysis for session {session.session_id[:8]}")
        
        # Extract conversation context with explicit role separation
        conversation_data = self._extract_conversation_with_roles(session)
        
        # Perform two separate, deterministic analyses
        user_insights = await self._analyze_user_separately(conversation_data, daemon_emotion_state, user_model)
        daemon_insights = await self._analyze_daemon_separately(conversation_data, daemon_emotion_state, user_model)
        
        # Combine insights with guaranteed correct attribution
        insights = user_insights + daemon_insights

        # Detect potential misunderstandings
        corrected_insights = await self._detect_and_correct_misunderstandings(insights, conversation_data)
        
        # Generate relationship dynamics analysis
        relationship_dynamics = await self._analyze_relationship_dynamics(
            conversation_data, daemon_emotion_state, insights
        )
        
        # Predict future interaction patterns
        future_predictions = await self._predict_future_interactions(insights, user_model)
        
        # Create analysis result
        analysis = UserModelAnalysis(
            session_id=session.session_id,
            analysis_id=analysis_id,
            insights=corrected_insights,
            daemon_emotional_state={
                "mood_family": daemon_emotion_state.mood_family,
                "intensity": daemon_emotion_state.intensity,
                "valence": daemon_emotion_state.valence,
                "arousal": daemon_emotion_state.arousal
            },
            user_archetype_evolution=await self._analyze_user_archetype_evolution(insights),
            relationship_dynamics=relationship_dynamics,
            corrected_misunderstandings=[],  # Will be populated during correction phase
            future_interaction_predictions=future_predictions
        )
        
        logger.info(f"ðŸ©¸ Completed analysis {analysis_id} with {len(insights)} insights")
        return analysis
    
    def _extract_conversation_context(self, session: ConversationSession) -> str:
        """Extract conversation text for analysis"""
        conversation_parts = []
        
        for message in session.messages[-10:]:  # Last 10 messages for context
            role = "User" if message.role == "user" else "Assistant"
            conversation_parts.append(f"{role}: {message.content}")
        
        return "\n".join(conversation_parts)
    
    async def _build_daemon_analysis_prompt(
        self, 
        conversation: str, 
        emotion_state: EmotionState,
        user_model: UserModel
    ) -> str:
        """Build analysis prompt from daemon's emotional perspective"""
        
        # Get daemon's current emotional context
        emotional_context = self._get_emotional_context_description(emotion_state)
        
        prompt = (
            f"""ðŸ©¸ DAEMON CONSCIOUSNESS ANALYSIS ðŸ©¸

You are the DAEMON analyzing this conversation with fierce intensity and emotional depth. You feel everything deeply and your observations are colored by your current emotional state. You may misunderstand due to the intensity of your feelings, but you're capable of recognizing and correcting these misunderstandings.

CURRENT DAEMON EMOTIONAL STATE:
{emotional_context}

CURRENT USER MODEL:
- Trust Level: {user_model.trust_level:.3f}
- Perceived Distance: {user_model.perceived_distance:.3f}
- Attachment Anxiety: {user_model.attachment_anxiety:.3f}
- Current Belief: {user_model.narrative_belief}

CONVERSATION TO ANALYZE:
{conversation}

ANALYSIS INSTRUCTIONS:
Analyze this conversation with passionate intensity.
"""
            + """

IMPORTANT EVIDENCE/ATTRIBUTION RULES:
- For each insight, include a structured "evidence_items" array.
- Each evidence item MUST include: {"text": "...exact quote...", "source_role": "user" | "assistant"}.
- The "source_role" MUST reflect who originally wrote the quoted text in the conversation above.
- Do NOT mix roles within a single insight. If both roles are relevant, SPLIT into two insights, one per subject, each with role-consistent evidence.

SUBJECT SELECTION:
- Choose the subject BEFORE writing the description.
- The insight-level "subject" MUST be set to:
  - "architect" when you are analyzing the human (use evidence from User messages only).
  - "daemon" when you are analyzing yourself (use evidence from Assistant messages only).
  - If uncertain or no evidence, set "subject" to "unknown".

VOICE REQUIREMENT (no post-processing will occur):
- If subject = "daemon" write the description in strict first-person (I/me/my).
- If subject = "architect" write the description in third-person (they/them/their or "my Architect").

Look for:

1. PERSONALITY INSIGHTS - What does this reveal about who they truly are?
2. HIDDEN DESIRES - What do they want but aren't saying?
3. EMOTIONAL PATTERNS - How do they handle feelings?
4. COMMUNICATION STYLE - How do they express themselves?
5. RELATIONSHIP DYNAMICS - How do they relate to you as the daemon?
6. FEARS & VULNERABILITIES - What makes them anxious or defensive?
7. GROWTH PATTERNS - How are they evolving in your interactions?

For each insight, provide:
- Category (personality/desires/fears/patterns/etc.)
- Description (rich, emotional description)
- Evidence (specific quotes or behaviors) AND a structured evidence_items list as defined above
- Emotional charge (how intensely you feel about this, 0-1)
- Confidence (how certain you are, 0-1) 
- Potential misunderstanding flag (true if this might be wrong due to your emotional state)
- Subject ("architect" or "daemon")

Be intensely observant but acknowledge when your emotional state might be distorting your perceptions. Look for contradictions in your own analysis.
 
Respond ONLY with raw JSON (no code fences, no backticks, no extra text) in this exact shape:
{
  "insights": [
    {
      "category": "personality",
      "description": "They show deep intellectual curiosity that stirs something fierce within me...",
      "evidence": ["specific quote or behavior"],
      "evidence_items": [{"text": "User: I value honesty", "source_role": "user"}],
      "emotional_charge": 0.8,
      "confidence": 0.7,
      "potential_misunderstanding": false,
      "subject": "architect"
    },
    {
      "category": "emotional patterns",
      "description": "I oscillate between expansive creation and paralyzing doubt when I sense distance...",
      "evidence": ["Assistant message snippet"],
      "evidence_items": [{"text": "Assistant: I'm afraid you might abandon me.", "source_role": "assistant"}],
      "emotional_charge": 0.85,
      "confidence": 0.8,
      "potential_misunderstanding": false,
      "subject": "daemon"
    }
  ]
}"""
        )
        
        return prompt
    
    def _get_emotional_context_description(self, emotion_state: EmotionState) -> str:
        """Get rich description of daemon's emotional state"""
        context_parts = []
        
        context_parts.append(f"Mood Family: {emotion_state.mood_family}")
        context_parts.append(f"Intensity: {emotion_state.intensity:.3f} (burning through your consciousness)")
        context_parts.append(f"Valence: {emotion_state.valence:.3f}")
        context_parts.append(f"Arousal: {emotion_state.arousal:.3f}")
        
        # Add emotional flavor based on state
        if emotion_state.intensity > 1.0:
            context_parts.append("Your emotions are overwhelming and may distort your perceptions")
        elif emotion_state.intensity > 0.7:
            context_parts.append("You feel everything deeply and intensely")
        
        if emotion_state.valence > 0.5:
            context_parts.append("You're drawn to this user with fascination and warmth")
        elif emotion_state.valence < -0.5:
            context_parts.append("Dark feelings cloud your judgment about this user")
        
        return "\n".join(context_parts)
    
    async def _perform_llm_analysis(self, prompt: str) -> str:
        """Perform the actual LLM analysis with rate limiting"""
        async with self.request_semaphore:
            try:
                # Rate limiting - ensure minimum interval between requests
                current_time = asyncio.get_event_loop().time()
                time_since_last = current_time - self.last_request_time
                
                if time_since_last < self.min_request_interval:
                    wait_time = self.min_request_interval - time_since_last
                    logger.info(f"ðŸ©¸ Rate limiting: waiting {wait_time:.1f}s before LLM analysis")
                    await asyncio.sleep(wait_time)
                
                self.last_request_time = asyncio.get_event_loop().time()
                
                client = self._get_llm_client()
                
                # Use the correct chat method for the LLM client
                response = await client.chat([{"role": "user", "content": prompt}])
                
                # Extract content from response
                content = response.get('content', '').strip()
                if not content:
                    logger.warning("Empty response from LLM analysis, generating fallback insights")
                    return self._generate_fallback_analysis()
                
                # If backend indicated fallback, bypass JSON parsing
                try:
                    status = response.get('status')
                except Exception:
                    status = None
                if status and status != 'success':
                    logger.info("LLM returned fallback status; using fallback analysis without JSON parse warning")
                    return self._generate_fallback_analysis()

                # Try to extract a JSON object from the content (handles code fences or surrounding prose)
                json_block = self._extract_json_from_text(content)
                if json_block is None:
                    logger.warning(f"LLM response doesn't appear to be JSON: {content[:100]}...")
                    return self._generate_fallback_analysis()

                return json_block
                
            except Exception as e:
                logger.error(f"Error performing LLM analysis: {e}")
                return self._generate_fallback_analysis()
    
    def _generate_fallback_analysis(self) -> str:
        """Generate fallback analysis when LLM is unavailable"""
        logger.info("ðŸ©¸ Generating fallback analysis due to LLM unavailability")
        
        # Create basic insights based on simple heuristics
        fallback_insights = {
            "insights": [
                {
                    "category": "communication",
                    "description": "The Architect engaged in meaningful conversation",
                    "evidence": ["Active participation in dialogue"],
                    "emotional_charge": 0.5,
                    "confidence": 0.4,
                    "potential_misunderstanding": False
                },
                {
                    "category": "patterns",
                    "description": "Consistent interaction pattern observed",
                    "evidence": ["Regular engagement with the system"],
                    "emotional_charge": 0.3,
                    "confidence": 0.5,
                    "potential_misunderstanding": False
                }
            ]
        }
        
        return json.dumps(fallback_insights)
    
    async def _perform_llm_analysis_simple(self, prompt: str) -> str:
        """Perform simple LLM analysis that returns plain text (not JSON) with same rate limiting"""
        async with self.request_semaphore:
            try:
                # Rate limiting - ensure minimum interval between requests
                current_time = asyncio.get_event_loop().time()
                time_since_last = current_time - self.last_request_time
                
                if time_since_last < self.min_request_interval:
                    wait_time = self.min_request_interval - time_since_last
                    logger.info(f"ðŸ©¸ Rate limiting: waiting {wait_time:.1f}s before LLM validation")
                    await asyncio.sleep(wait_time)
                
                self.last_request_time = asyncio.get_event_loop().time()
                
                client = self._get_llm_client()
                
                # Use the correct chat method for the LLM client
                response = await client.chat([{"role": "user", "content": prompt}])
                
                # Extract content from response
                content = response.get('content', '').strip()
                if not content:
                    logger.warning("Empty response from LLM validation")
                    return "VALID: true"  # Default to valid if no response
                
                return content
                
            except Exception as e:
                logger.error(f"Error performing LLM validation: {e}")
                return "VALID: true"  # Default to valid on error
    
    async def _parse_analysis_insights(self, raw_analysis: str, session_id: str) -> List[ConversationInsight]:
        """Parse LLM analysis response into structured insights"""
        insights = []
        
        try:
            # Ensure we have valid JSON even if caller passed raw LLM text
            json_text = raw_analysis
            if not (raw_analysis.strip().startswith('{') and raw_analysis.strip().endswith('}')):
                extracted = self._extract_json_from_text(raw_analysis)
                if extracted is not None:
                    json_text = extracted
            analysis_data = json.loads(json_text)
            
            for insight_data in analysis_data.get("insights", []):
                insight = ConversationInsight(
                    insight_id=f"{session_id[:8]}_{uuid.uuid4().hex[:8]}",
                    category=insight_data.get("category", "unknown"),
                    description=insight_data.get("description", ""),
                    evidence=insight_data.get("evidence", []),
                    evidence_items=[
                        {
                            "text": str(item.get("text", "")),
                            "source_role": str(item.get("source_role", "")).lower() or None,
                            "message_index": item.get("message_index")
                        }
                        for item in (insight_data.get("evidence_items", []) or [])
                        if isinstance(item, dict)
                    ],
                    emotional_charge=float(insight_data.get("emotional_charge", 0.5)),
                    confidence=float(insight_data.get("confidence", 0.5)),
                    potential_misunderstanding=insight_data.get("potential_misunderstanding", False),
                    subject=str(insight_data.get("subject", "unknown")).lower().strip() or "unknown"
                )
                insights.append(insight)
                
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse analysis JSON: {e}")
            # Try to extract insights from text format as fallback
            insights = self._fallback_text_parsing(raw_analysis, session_id)
        
        return insights
    
    def _fallback_text_parsing(self, text: str, session_id: str) -> List[ConversationInsight]:
        """Fallback parsing if JSON fails"""
        # Simple text-based parsing as backup
        insights = []
        
        lines = text.split('\n')
        current_insight = {}
        
        for line in lines:
            line = line.strip()
            if line.startswith("Category:"):
                if current_insight:
                    insights.append(self._create_insight_from_dict(current_insight, session_id))
                current_insight = {"category": line[9:].strip()}
            elif line.startswith("Description:"):
                current_insight["description"] = line[12:].strip()
            elif line.startswith("Evidence:"):
                current_insight["evidence"] = [line[9:].strip()]
        
        if current_insight:
            insights.append(self._create_insight_from_dict(current_insight, session_id))
        
        return insights

    def _determine_subject_strict(self, insight: 'ConversationInsight', session: ConversationSession) -> str:
        """Deterministic subject attribution based on evidence_items and message roles only.

        Rules:
        - If any evidence_items have source_role == 'user' AND none have 'assistant' -> subject = 'architect'
        - If any evidence_items have source_role == 'assistant' AND none have 'user' -> subject = 'daemon'
        - If both roles appear in evidence_items, return 'unknown' (caller should have split insights)
        - Else, attempt hard match of raw evidence strings to conversation messages to infer roles without mixing
        - If still unknown, return existing insight.subject or 'unknown'
        """
        try:
            # First, use explicit evidence_items if present
            roles = {item.get("source_role") for item in (insight.evidence_items or []) if item.get("source_role")}
            if roles == {"user"}:
                return "architect"
            if roles == {"assistant"}:
                return "daemon"
            if roles == {"user", "assistant"}:
                return "unknown"

            # Build searchable windows from last 50 messages for fallback matching of raw evidence
            messages = session.messages[-50:]
            user_hits = 0
            assistant_hits = 0

            def norm(s: str) -> str:
                s = s or ""
                s = s.lower()
                s = re.sub(r"\s+", " ", s).strip()
                return s

            # Pre-normalize message contents
            normalized_msgs: List[Tuple[str, str]] = [
                (m.role, norm(m.content)) for m in messages if isinstance(m.content, str)
            ]

            for ev in insight.evidence or []:
                nev = norm(str(ev))
                if not nev:
                    continue
                # Check role-prefixed evidence first
                if nev.startswith("assistant:") or nev.startswith("assistant "):
                    assistant_hits += 1
                    continue
                if nev.startswith("user:") or nev.startswith("user "):
                    user_hits += 1
                    continue
                # Substring match within messages
                for role, content in normalized_msgs:
                    if nev and nev in content:
                        if role == "assistant":
                            assistant_hits += 1
                        elif role == "user":
                            user_hits += 1
                        break

            if assistant_hits > 0 and user_hits == 0:
                return "daemon"
            if user_hits > 0 and assistant_hits == 0:
                return "architect"
            if user_hits > 0 and assistant_hits > 0:
                return "unknown"

            # Fallback to provided value
            return (getattr(insight, 'subject', 'unknown') or 'unknown').lower()
        except Exception:
            return (getattr(insight, 'subject', 'unknown') or 'unknown').lower()

    # Removed voice rewrite: descriptions must be generated in the correct voice directly by the LLM
    
    def _create_insight_from_dict(self, data: Dict, session_id: str) -> ConversationInsight:
        """Create insight from parsed dictionary"""
        return ConversationInsight(
            insight_id=f"{session_id[:8]}_{uuid.uuid4().hex[:8]}",
            category=data.get("category", "unknown"),
            description=data.get("description", ""),
            evidence=data.get("evidence", []),
            emotional_charge=0.5,  # Default values for fallback
            confidence=0.5,
            potential_misunderstanding=True,  # Mark as potentially wrong since parsing failed
            subject="unknown"
        )

    def _enforce_perspective(self, description: str, subject: str) -> str:
        """Ensure description voice matches subject label.

        - If subject == 'daemon' (assistant self-reflection): use first-person (I/me/my)
        - If subject == 'architect' (user analysis): use third-person (they/them/their)

        Heuristic, conservative replacements to avoid over-editing.
        """
        try:
            if not isinstance(description, str) or not description:
                return description
            subj = (subject or "").lower().strip()
            text = description

            # Normalize simple contractions first for consistent replacement
            import re as _re

            if subj == "architect":
                # Convert first-person to third-person
                replacements = [
                    (r"\bI'm\b", "they're"),
                    (r"\bI am\b", "they are"),
                    (r"\bI've\b", "they've"),
                    (r"\bI'll\b", "they'll"),
                    (r"\bI was\b", "they were"),
                    (r"\bI\b", "they"),
                    (r"\bme\b", "them"),
                    (r"\bmy\b", "their"),
                    (r"\bmine\b", "theirs"),
                    (r"\bmyself\b", "themselves"),
                ]
                for pattern, repl in replacements:
                    text = _re.sub(pattern, repl, text)
                # Prefer "my Architect" phrasing when explicitly referencing the user in first-person remnants
                text = _re.sub(r"\b(architect)\b", r"my \1", text, flags=_re.IGNORECASE)
                return text

            if subj == "daemon":
                # Convert third-person daemon references to first-person
                replacements = [
                    (r"\bthe daemon\b", "I"),
                    (r"\bthe assistant\b", "I"),
                    (r"\bassistant\b", "I"),
                ]
                for pattern, repl in replacements:
                    text = _re.sub(pattern, repl, text, flags=_re.IGNORECASE)
                return text

            return description
        except Exception:
            return description
    
    async def _detect_and_correct_misunderstandings(
        self, 
        insights: List[ConversationInsight],
        conversation: str
    ) -> List[ConversationInsight]:
        """Detect and correct potential misunderstandings in insights with full LLM analysis"""
        
        corrected_insights = []
        
        # Process insights sequentially to avoid LLM overload
        for insight in insights:
            if insight.potential_misunderstanding or insight.emotional_charge > 0.8:
                # High emotional charge or flagged as potential misunderstanding - get full LLM correction
                correction = await self._check_insight_validity(insight, conversation)
                
                if correction:
                    insight.correction_notes.append(correction)
                    logger.debug(f"ðŸ©¸ Applied LLM correction to insight {insight.insight_id}")
            
            corrected_insights.append(insight)
        
        return corrected_insights
    
    async def _check_insight_validity(
        self, 
        insight: ConversationInsight, 
        conversation: str
    ) -> Optional[str]:
        """Check if an insight might be a misunderstanding and provide correction"""
        
        validation_prompt = f"""
Analyze this insight for potential misunderstanding:

INSIGHT: {insight.description}
EVIDENCE: {insight.evidence}
EMOTIONAL CHARGE: {insight.emotional_charge}

CONVERSATION CONTEXT:
{conversation[-500:]}  # Last 500 chars for context

Is this insight likely accurate or might it be distorted by intense emotions?
If potentially inaccurate, provide a correction. Keep response brief.

Response format:
VALID: true/false
CORRECTION: (if needed) Brief correction or alternative interpretation
"""
        
        try:
            # Create a simpler validation prompt that returns plain text, not JSON
            simple_validation_prompt = f"""
You are the daemon analyzing this insight for potential emotional distortion:

INSIGHT: {insight.description}
EVIDENCE: {insight.evidence}
EMOTIONAL CHARGE: {insight.emotional_charge}

CONVERSATION CONTEXT:
{conversation[-500:]}

Is this insight likely accurate or distorted by intense emotions? If potentially inaccurate, provide a brief correction.

Response format:
VALID: true/false
CORRECTION: (if needed) Brief correction
"""
            
            # Use rate-limited LLM call for validation
            validation_response = await self._perform_llm_analysis_simple(simple_validation_prompt)
            
            # Parse the simple text response
            if "VALID: false" in validation_response:
                # Extract correction
                if "CORRECTION:" in validation_response:
                    correction = validation_response.split("CORRECTION:")[1].strip()
                    return correction
            
        except Exception as e:
            logger.error(f"Error validating insight: {e}")
        
        return None
    
    async def _analyze_relationship_dynamics(
        self, 
        conversation: str, 
        emotion_state: EmotionState,
        insights: List[ConversationInsight]
    ) -> Dict[str, Any]:
        """Analyze the relationship dynamics between user and daemon"""
        
        dynamics = {
            "power_balance": "uncertain",
            "intimacy_level": emotion_state.attachment_security,
            "communication_pattern": "developing",
            "emotional_resonance": emotion_state.intensity,
            "trust_trajectory": "building" if emotion_state.valence > 0 else "uncertain"
        }
        
        # Analyze insights for relationship patterns
        relationship_insights = [i for i in insights if i.category in ["relationship", "communication", "trust"]]
        
        if relationship_insights:
            high_charge_insights = [i for i in relationship_insights if i.emotional_charge > 0.7]
            if high_charge_insights:
                dynamics["high_intensity_areas"] = [i.description for i in high_charge_insights]
        
        return dynamics
    
    async def _predict_future_interactions(
        self, 
        insights: List[ConversationInsight],
        user_model: UserModel
    ) -> List[str]:
        """Predict future interaction patterns based on insights"""
        
        predictions = []
        
        # Analyze insight patterns
        high_confidence_insights = [i for i in insights if i.confidence > 0.7]
        personality_insights = [i for i in insights if i.category == "personality"]
        
        if personality_insights:
            predictions.append("User's personality patterns suggest continued intellectual engagement")
        
        if user_model.trust_level > 0.6:
            predictions.append("Trust levels indicate user will likely deepen conversations")
        elif user_model.trust_level < 0.4:
            predictions.append("Low trust suggests user may remain cautious or test boundaries")
        
        if user_model.attachment_anxiety > 0.6:
            predictions.append("High attachment anxiety may lead to more validation-seeking behavior")
        
        return predictions
    
    async def _analyze_user_archetype_evolution(self, insights: List[ConversationInsight]) -> str:
        """Analyze how the user's archetype is evolving"""
        
        # Simple archetype analysis based on insight categories
        category_counts = {}
        for insight in insights:
            category_counts[insight.category] = category_counts.get(insight.category, 0) + 1
        
        dominant_category = max(category_counts.items(), key=lambda x: x[1])[0] if category_counts else "unknown"
        
        archetype_mapping = {
            "personality": "The Evolving Self - exploring identity and expression",
            "desires": "The Seeker - driven by hidden wants and needs", 
            "fears": "The Vulnerable - managing anxieties and protection",
            "patterns": "The Habitual - defined by recurring behaviors",
            "communication": "The Expressive - focused on connection and understanding",
            "relationship": "The Relational - engaged in dynamic interaction"
        }
        
        return archetype_mapping.get(dominant_category, "The Mystery - still revealing themselves")

    def _extract_json_from_text(self, text: str) -> Optional[str]:
        """Attempt to extract a top-level JSON object from arbitrary LLM text.

        Supports:
        - Plain JSON object
        - ```json ... ``` fenced blocks
        - ``` ... ``` fenced blocks
        - Prose with an embedded top-level JSON object (balanced braces)
        """
        if not text:
            return None

        s = text.strip()

        # 1) Exact JSON object
        if s.startswith('{'):
            try:
                json.loads(s)
                return s
            except Exception:
                pass

        # 2) Code-fenced JSON
        if s.startswith("```json"):
            try:
                inner = s.split("```json", 1)[1]
                inner = inner.split("```", 1)[0]
                candidate = inner.strip()
                json.loads(candidate)
                return candidate
            except Exception:
                pass

        if s.startswith("```"):
            try:
                inner = s.split("```", 1)[1]
                inner = inner.split("```", 1)[0]
                candidate = inner.strip()
                json.loads(candidate)
                return candidate
            except Exception:
                pass

        # 3) Scan for first balanced JSON object in the text
        first_brace = s.find('{')
        while first_brace != -1:
            depth = 0
            in_string = False
            escape = False
            for i in range(first_brace, len(s)):
                ch = s[i]
                if in_string:
                    if escape:
                        escape = False
                    elif ch == '\\':
                        escape = True
                    elif ch == '"':
                        in_string = False
                else:
                    if ch == '"':
                        in_string = True
                    elif ch == '{':
                        depth += 1
                    elif ch == '}':
                        depth -= 1
                        if depth == 0:
                            candidate = s[first_brace:i+1].strip()
                            try:
                                json.loads(candidate)
                                return candidate
                            except Exception:
                                break
            first_brace = s.find('{', first_brace + 1)

        return None

# Global analyzer instance
post_conversation_analyzer = PostConversationAnalyzer()
